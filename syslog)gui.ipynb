{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fd95cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py\", line 1921, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_4064\\4195535685.py\", line 69, in print_matches\n",
      "    new_tfidf_matrix = TfidfVectorizer(vocabulary=tfidf_matrix.get_feature_names()).fit_transform(new_tokens)\n",
      "AttributeError: 'csr_matrix' object has no attribute 'get_feature_names'\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "class LogAnalyzerGUI:\n",
    "    def __init__(self):\n",
    "        self.window = tk.Tk()\n",
    "        self.window.title(\"Log Analyzer\")\n",
    "        self.window.geometry(\"400x300\")\n",
    "\n",
    "        # Set GUI colors and fonts\n",
    "        self.window.configure(bg=\"#f2f2f2\")\n",
    "        self.label_bg_color = \"#f2f2f2\"\n",
    "        self.button_bg_color = \"#336699\"\n",
    "        self.button_fg_color = \"#ffffff\"\n",
    "        self.result_text_color = \"#333333\"\n",
    "        self.result_text_font = (\"Arial\", 10)\n",
    "\n",
    "        # Buttons\n",
    "        self.btn_create_model = self.create_button(\"Model Creation and Training\", self.create_model)\n",
    "        self.btn_use_saved_model = self.create_button(\"Use Saved Model and Test\", self.use_saved_model)\n",
    "        self.btn_further_training = self.create_button(\"Further Training of the Model\", self.further_training)\n",
    "        self.btn_back_to_menu = self.create_button(\"Back to Menu\", self.back_to_menu)\n",
    "\n",
    "        self.window.mainloop()\n",
    "\n",
    "    def create_button(self, text, command):\n",
    "        button = tk.Button(self.window, text=text, bg=self.button_bg_color, fg=self.button_fg_color, command=command)\n",
    "        button.pack(pady=10)\n",
    "        return button\n",
    "\n",
    "    def clear_window(self):\n",
    "        for widget in self.window.winfo_children():\n",
    "            widget.destroy()\n",
    "\n",
    "    def create_model(self):\n",
    "        self.clear_window()\n",
    "\n",
    "        # Select Training File\n",
    "        self.training_file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        self.label_training_file = self.create_label(\"Training File: \" + self.training_file_path)\n",
    "\n",
    "        # Select Test File\n",
    "        self.test_file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        self.label_test_file = self.create_label(\"Test File: \" + self.test_file_path)\n",
    "\n",
    "        # Print Matches\n",
    "        self.btn_print_matches = self.create_button(\"Print Matches\", self.print_matches)\n",
    "        self.btn_clear_results = self.create_button(\"Clear Results\", self.clear_results)\n",
    "\n",
    "    def create_label(self, text):\n",
    "        label = tk.Label(self.window, text=text, bg=self.label_bg_color)\n",
    "        label.pack(pady=5)\n",
    "        return label\n",
    "\n",
    "    def print_matches(self):\n",
    "        data = [content for content in self.get_log_contents(self.training_file_path)]\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        tokens = [' '.join([token.text for token in doc]) for doc in nlp.pipe(data, batch_size=1000, disable=[\"parser\", \"ner\"])]\n",
    "        tfidf_matrix = TfidfVectorizer().fit_transform(tokens)\n",
    "\n",
    "        new_data = [content for content in self.get_log_contents(self.test_file_path)]\n",
    "        new_tokens = [' '.join([token.text for token in doc]) for doc in nlp.pipe(new_data, batch_size=1000, disable=[\"parser\", \"ner\"])]\n",
    "        new_tfidf_matrix = TfidfVectorizer(vocabulary=tfidf_matrix.get_feature_names()).fit_transform(new_tokens)\n",
    "\n",
    "        similarity_scores = cosine_similarity(new_tfidf_matrix, tfidf_matrix)\n",
    "        threshold = 0.66\n",
    "\n",
    "        self.results_text = self.create_text_widget()\n",
    "        for i, data_point in enumerate(new_data):\n",
    "            max_similarity = max(similarity_scores[i])\n",
    "            found_match = False\n",
    "            if max_similarity >= threshold:\n",
    "                index = similarity_scores[i].argmax()\n",
    "                matched_data = data[index]\n",
    "                if data_point != matched_data:\n",
    "                    result = f\"New Data: {data_point}\\nSimilar Data: {matched_data}\\nSimilarity Score: {max_similarity}\\n\\n\"\n",
    "                    self.results_text.insert(tk.END, result)\n",
    "\n",
    "            if not found_match:\n",
    "                result = f\"New Data: {data_point}\\nNo similar data found.\\n\\n\"\n",
    "                self.results_text.insert(tk.END, result)\n",
    "\n",
    "    def use_saved_model(self):\n",
    "        self.clear_window()\n",
    "\n",
    "        # Select Saved Model\n",
    "        self.saved_model_path = filedialog.askopenfilename(filetypes=[(\"Pickle Files\", \"*.pkl\")])\n",
    "        self.label_saved_model = self.create_label(\"Saved Model: \" + self.saved_model_path)\n",
    "\n",
    "        # Select New Test File\n",
    "        self.new_test_file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        self.label_new_test_file = self.create_label(\"New Test File: \" + self.new_test_file_path)\n",
    "\n",
    "        # Test Saved Model Again\n",
    "        self.btn_test_saved_model = self.create_button(\"Test Saved Model\", self.test_saved_model)\n",
    "        self.btn_clear_results = self.create_button(\"Clear Results\", self.clear_results)\n",
    "\n",
    "    def test_saved_model(self):\n",
    "        vectorizer = joblib.load(self.saved_model_path)\n",
    "\n",
    "        new_data = [content for content in self.get_log_contents(self.new_test_file_path)]\n",
    "        new_tokens = [' '.join([token.text for token in doc]) for doc in nlp.pipe(new_data, batch_size=1000, disable=[\"parser\", \"ner\"])]\n",
    "        new_tfidf_matrix = vectorizer.transform(new_tokens)\n",
    "\n",
    "        similarity_scores = cosine_similarity(new_tfidf_matrix, vectorizer.transform(vectorizer.get_feature_names()))\n",
    "        threshold = 0.66\n",
    "\n",
    "        self.results_text = self.create_text_widget()\n",
    "        for i, data_point in enumerate(new_data):\n",
    "            max_similarity = max(similarity_scores[i])\n",
    "            found_match = False\n",
    "            if max_similarity >= threshold:\n",
    "                index = similarity_scores[i].argmax()\n",
    "                matched_data = vectorizer.get_feature_names()[index]\n",
    "                if data_point != matched_data:\n",
    "                    result = f\"New Data: {data_point}\\nSimilar Data: {matched_data}\\nSimilarity Score: {max_similarity}\\n\\n\"\n",
    "                    self.results_text.insert(tk.END, result)\n",
    "\n",
    "            if not found_match:\n",
    "                result = f\"New Data: {data_point}\\nNo similar data found.\\n\\n\"\n",
    "                self.results_text.insert(tk.END, result)\n",
    "\n",
    "    def further_training(self):\n",
    "        self.clear_window()\n",
    "\n",
    "        # Select Model for Further Training\n",
    "        self.model_for_training_path = filedialog.askopenfilename(filetypes=[(\"Pickle Files\", \"*.pkl\")])\n",
    "        self.label_model_for_training = self.create_label(\"Model for Training: \" + self.model_for_training_path)\n",
    "\n",
    "        # Select Additional Training Data\n",
    "        self.additional_training_data_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        self.label_additional_training_data = self.create_label(\"Additional Training Data: \" + self.additional_training_data_path)\n",
    "\n",
    "        # Train Model with Additional Data\n",
    "        self.btn_train_model = self.create_button(\"Train Model with Additional Data\", self.train_model)\n",
    "        self.btn_back_to_menu = self.create_button(\"Back to Menu\", self.back_to_menu)\n",
    "\n",
    "    def train_model(self):\n",
    "        vectorizer = joblib.load(self.model_for_training_path)\n",
    "\n",
    "        additional_training_data = [content for content in self.get_log_contents(self.additional_training_data_path)]\n",
    "        additional_tokens = [' '.join([token.text for token in doc]) for doc in nlp.pipe(additional_training_data, batch_size=1000, disable=[\"parser\", \"ner\"])]\n",
    "        tfidf_matrix = vectorizer.transform(additional_tokens)\n",
    "\n",
    "        joblib.dump(vectorizer, 'trained_model.pkl')\n",
    "        sparse.save_npz('tfidf_matrix.npz', tfidf_matrix)\n",
    "\n",
    "    def clear_results(self):\n",
    "        self.results_text.delete(\"1.0\", tk.END)\n",
    "\n",
    "    def get_log_contents(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return [re.sub(r'^\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2} [\\w.-]+ kernel:', '', line.strip()) for line in file if re.match(r'\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2}', line.strip())]\n",
    "\n",
    "    def create_text_widget(self):\n",
    "        text_widget = tk.Text(self.window, bg=self.label_bg_color, fg=self.result_text_color, font=self.result_text_font)\n",
    "        text_widget.pack(pady=10)\n",
    "        return text_widget\n",
    "\n",
    "    def back_to_menu(self):\n",
    "        self.clear_window()\n",
    "        self.__init__()\n",
    "\n",
    "# Create the GUI\n",
    "log_analyzer_gui = LogAnalyzerGUI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1da40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cece0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b0df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee8192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef2f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f9de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2975a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f61378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a406a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74baa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "393f616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter.scrolledtext import ScrolledText\n",
    "\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if re.match(r'\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2}', line):\n",
    "                content = re.sub(r'^\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2} [\\w.-]+ kernel:', '', line).strip()\n",
    "                if content:\n",
    "                    data.append(content)\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_model(data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    tokens = []\n",
    "\n",
    "    for doc in nlp.pipe(data, batch_size=1000, disable=[\"parser\", \"ner\"]):\n",
    "        doc_tokens = [token.text for token in doc]\n",
    "        tokens.append(' '.join(doc_tokens))\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(tokens)\n",
    "\n",
    "    # Save trained model\n",
    "    joblib.dump(vectorizer, 'trained_model.pkl')\n",
    "    sparse.save_npz('tfidf_matrix.npz', tfidf_matrix)\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    try:\n",
    "        # Load trained model\n",
    "        vectorizer = joblib.load('trained_model.pkl')\n",
    "        tfidf_matrix = sparse.load_npz('tfidf_matrix.npz')\n",
    "        return vectorizer, tfidf_matrix\n",
    "    except FileNotFoundError:\n",
    "        messagebox.showerror(\"Error\", \"Trained model and TF-IDF matrix not found. Run training step first.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def process_new_data(new_data_file, vectorizer, tfidf_matrix):\n",
    "    results = []\n",
    "    with open(new_data_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            # Ignore date and time\n",
    "            if re.match(r'\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2}', line):\n",
    "                content = re.sub(r'^\\w{3} \\d{2} \\d{2}:\\d{2}:\\d{2} [\\w.-]+ kernel:', '', line).strip()\n",
    "                if content:\n",
    "                    new_tfidf_matrix = vectorizer.transform([content])\n",
    "                    similarity_scores = cosine_similarity(new_tfidf_matrix, tfidf_matrix)\n",
    "                    max_similarity = similarity_scores.max()\n",
    "                    threshold = 0.66  # Adjust the threshold based on your requirements\n",
    "                    if max_similarity >= threshold:\n",
    "                        index = similarity_scores.argmax()\n",
    "                        matched_data = data[index]\n",
    "                        if content != matched_data:\n",
    "                            result = f\"New Data: {content}\\nSimilar Data: {matched_data}\\nSimilarity Score: {max_similarity}\\n\"\n",
    "                            results.append(result)\n",
    "                    else:\n",
    "                        result = f\"New Data: {content}\\nNo similar data found.\\n\"\n",
    "                        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def select_data_file(entry):\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "    if file_path:\n",
    "        entry.delete(0, tk.END)\n",
    "        entry.insert(tk.END, file_path)\n",
    "\n",
    "\n",
    "def show_results():\n",
    "    data_file = data_file_entry.get()\n",
    "    new_data_file = new_data_file_entry.get()\n",
    "\n",
    "    if not data_file or not new_data_file:\n",
    "        messagebox.showwarning(\"Warning\", \"Please select both data files.\")\n",
    "        return\n",
    "\n",
    "    data = preprocess_data(data_file)\n",
    "    train_model(data)\n",
    "    vectorizer, tfidf_matrix = load_trained_model()\n",
    "\n",
    "    if vectorizer is not None and tfidf_matrix is not None:\n",
    "        results = process_new_data(new_data_file, vectorizer, tfidf_matrix)\n",
    "        if results:\n",
    "            results_window = tk.Toplevel()\n",
    "            results_window.title(\"Text Similarity Results\")\n",
    "            results_window.geometry(\"500x500\")\n",
    "            results_window.configure(background=\"#F0F0F0\")\n",
    "\n",
    "            results_text = ScrolledText(results_window, height=20, width=60, font=(\"Roboto\", 12))\n",
    "            results_text.pack(pady=20, padx=20)\n",
    "\n",
    "            for result in results:\n",
    "                results_text.insert(tk.END, result + '\\n')\n",
    "            results_text.config(state=tk.DISABLED)\n",
    "\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Text Similarity\")\n",
    "window.geometry(\"500x150\")\n",
    "window.configure(background=\"#F0F0F0\")\n",
    "\n",
    "# Create and place widgets\n",
    "data_frame = tk.Frame(window, bg=\"#F0F0F0\")\n",
    "data_frame.pack(pady=20, padx=20)\n",
    "\n",
    "data_file_label = tk.Label(data_frame, text=\"Select Data File:\", bg=\"#F0F0F0\", fg=\"#333333\", font=(\"Roboto\", 12))\n",
    "data_file_label.grid(row=0, column=0, pady=(0, 5), sticky=\"w\")\n",
    "\n",
    "data_file_entry = tk.Entry(data_frame, width=50, font=(\"Roboto\", 12))\n",
    "data_file_entry.grid(row=1, column=0, pady=(0, 10), padx=5, sticky=\"w\")\n",
    "\n",
    "data_file_button = tk.Button(data_frame, text=\"Browse\", command=lambda: select_data_file(data_file_entry),\n",
    "                             font=(\"Roboto\", 12))\n",
    "data_file_button.grid(row=1, column=1, pady=(0, 10), padx=5, sticky=\"e\")\n",
    "\n",
    "new_data_file_label = tk.Label(data_frame, text=\"Select New Data File:\", bg=\"#F0F0F0\", fg=\"#333333\",\n",
    "                               font=(\"Roboto\", 12))\n",
    "new_data_file_label.grid(row=2, column=0, pady=(0, 5), sticky=\"w\")\n",
    "\n",
    "new_data_file_entry = tk.Entry(data_frame, width=50, font=(\"Roboto\", 12))\n",
    "new_data_file_entry.grid(row=3, column=0, pady=(0, 10), padx=5, sticky=\"w\")\n",
    "\n",
    "new_data_file_button = tk.Button(data_frame, text=\"Browse\", command=lambda: select_data_file(new_data_file_entry),\n",
    "                                 font=(\"Roboto\", 12))\n",
    "new_data_file_button.grid(row=3, column=1, pady=(0, 10), padx=5, sticky=\"e\")\n",
    "\n",
    "results_button = tk.Button(window, text=\"Show Results\", command=show_results, font=(\"Roboto\", 12),\n",
    "                           bg=\"#333333\", fg=\"#FFFFFF\")\n",
    "results_button.pack(pady=(0, 10), padx=20)\n",
    "\n",
    "# Start the main loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3093e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d0c745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db0755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501e37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab279ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
